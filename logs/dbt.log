

============================== 2023-02-21 23:24:50.787631 | e12cec22-a3cf-4980-aaba-7db07baddd57 ==============================
[0m23:24:50.787631 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:24:50.788412 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m23:24:50.788541 [debug] [MainThread]: Tracking: tracking
[0m23:24:50.798859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10830d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108341490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108349550>]}
[0m23:24:51.282946 [debug] [MainThread]: Executing "git --help"
[0m23:24:51.289380 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:24:51.289779 [debug] [MainThread]: STDERR: "b''"
[0m23:24:51.293935 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m23:24:51.294428 [debug] [MainThread]: Using databricks connection "debug"
[0m23:24:51.321161 [debug] [MainThread]: On debug: select 1 as id
[0m23:24:51.321412 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:24:52.150262 [debug] [MainThread]: SQL status: OK in 0.83 seconds
[0m23:24:52.152484 [debug] [MainThread]: On debug: Close
[0m23:24:52.415018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076e4c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2b4580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2b4880>]}
[0m23:24:52.416033 [debug] [MainThread]: Flushing usage events
[0m23:24:52.980704 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2023-02-21 23:30:43.726227 | 800e7e57-d0f3-4636-af9e-26ba3d59f31f ==============================
[0m23:30:43.726227 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:30:43.727865 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:30:43.728049 [debug] [MainThread]: Tracking: tracking
[0m23:30:43.739244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a71c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a7640>]}
[0m23:30:43.784496 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m23:30:43.784777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268334f0>]}
[0m23:30:44.254043 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m23:30:44.260579 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:30:44.262538 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:30:44.300025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b2b970>]}
[0m23:30:44.303474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b43100>]}
[0m23:30:44.303654 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:30:44.303880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b92e50>]}
[0m23:30:44.304403 [info ] [MainThread]: 
[0m23:30:44.305256 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:30:44.305734 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:30:44.311147 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:30:44.311369 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:30:44.311501 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:30:44.929648 [debug] [ThreadPool]: SQL status: OK in 0.62 seconds
[0m23:30:44.942414 [debug] [ThreadPool]: On list_schemas: Close
[0m23:30:45.103771 [debug] [ThreadPool]: Acquiring new databricks connection 'create__dbt_silver'
[0m23:30:45.105082 [debug] [ThreadPool]: Acquiring new databricks connection 'create__dbt_silver'
[0m23:30:45.105792 [debug] [ThreadPool]: Creating schema "ReferenceKeyMsg(database=None, schema='dbt_silver', identifier=None)"
[0m23:30:45.116022 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:45.116449 [debug] [ThreadPool]: Using databricks connection "create__dbt_silver"
[0m23:30:45.116821 [debug] [ThreadPool]: On create__dbt_silver: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "create__dbt_silver"} */
create schema if not exists `dbt_silver`
  
[0m23:30:45.117177 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:30:46.028191 [debug] [ThreadPool]: SQL status: OK in 0.91 seconds
[0m23:30:46.030964 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m23:30:46.031633 [debug] [ThreadPool]: On create__dbt_silver: ROLLBACK
[0m23:30:46.032131 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:30:46.032590 [debug] [ThreadPool]: On create__dbt_silver: Close
[0m23:30:46.190971 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_dbt_silver'
[0m23:30:46.216225 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:46.216549 [debug] [ThreadPool]: Using databricks connection "list_None_dbt_silver"
[0m23:30:46.216751 [debug] [ThreadPool]: On list_None_dbt_silver: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_dbt_silver"} */
show tables in `dbt_silver`
  
[0m23:30:46.217164 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:30:46.809179 [debug] [ThreadPool]: SQL status: OK in 0.59 seconds
[0m23:30:46.815341 [debug] [ThreadPool]: On list_None_dbt_silver: ROLLBACK
[0m23:30:46.815854 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:30:46.816237 [debug] [ThreadPool]: On list_None_dbt_silver: Close
[0m23:30:46.964481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b63b80>]}
[0m23:30:46.965500 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:46.965962 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:30:46.967017 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:30:46.967677 [info ] [MainThread]: 
[0m23:30:46.973086 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m23:30:46.973673 [info ] [Thread-1  ]: 1 of 1 START sql table model dbt_silver.silver_cities .......................... [RUN]
[0m23:30:46.974763 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m23:30:46.975093 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m23:30:46.978257 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m23:30:46.979224 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-21 23:30:46.975349 => 2023-02-21 23:30:46.979159
[0m23:30:46.979551 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m23:30:47.022599 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m23:30:47.023109 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:47.023275 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m23:30:47.023439 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `dbt_silver`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from bronze_cities
order by id 
limit 5
  
[0m23:30:47.023582 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:30:47.755025 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `dbt_silver`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from bronze_cities
order by id 
limit 5
  
[0m23:30:47.755970 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
[0m23:30:47.756405 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:569)
	... 19 more

[0m23:30:47.756847 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2?\xc4\xe1\x16\xf9\x8e\xe3\x12?\xa6\xf4K\xf7'
[0m23:30:47.757792 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-21 23:30:46.979795 => 2023-02-21 23:30:47.757610
[0m23:30:47.758357 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m23:30:47.758857 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:30:47.759306 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m23:30:47.954263 [debug] [Thread-1  ]: Runtime Error in model silver_cities (models/silver_cities.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
[0m23:30:47.954773 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '800e7e57-d0f3-4636-af9e-26ba3d59f31f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126c4efa0>]}
[0m23:30:47.955218 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model dbt_silver.silver_cities ................. [[31mERROR[0m in 0.98s]
[0m23:30:47.956434 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m23:30:47.957403 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:30:47.957643 [debug] [MainThread]: On master: ROLLBACK
[0m23:30:47.957875 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:30:48.148222 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:30:48.149597 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:30:48.150145 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:30:48.150720 [debug] [MainThread]: On master: ROLLBACK
[0m23:30:48.151187 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:30:48.151630 [debug] [MainThread]: On master: Close
[0m23:30:48.318373 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:30:48.319325 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m23:30:48.321905 [info ] [MainThread]: 
[0m23:30:48.322659 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.02 seconds (4.02s).
[0m23:30:48.323540 [debug] [MainThread]: Command end result
[0m23:30:48.337862 [info ] [MainThread]: 
[0m23:30:48.338483 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:30:48.338915 [info ] [MainThread]: 
[0m23:30:48.339340 [error] [MainThread]: [33mRuntime Error in model silver_cities (models/silver_cities.sql)[0m
[0m23:30:48.339753 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze_cities` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m23:30:48.340171 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m23:30:48.340571 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 18 pos 30
[0m23:30:48.340975 [info ] [MainThread]: 
[0m23:30:48.341383 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:30:48.342053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b13190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b5bd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126b92e50>]}
[0m23:30:48.342352 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:32:48.083317 | 1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea ==============================
[0m23:32:48.083317 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:32:48.084938 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:32:48.085121 [debug] [MainThread]: Tracking: tracking
[0m23:32:48.103712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152ace1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152ace670>]}
[0m23:32:48.150856 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m23:32:48.151178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a75a60>]}
[0m23:32:48.611278 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m23:32:48.617812 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:32:48.619727 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:32:48.656771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d7d0d0>]}
[0m23:32:48.660541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152b3d580>]}
[0m23:32:48.660724 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:32:48.660932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d9f5e0>]}
[0m23:32:48.661449 [info ] [MainThread]: 
[0m23:32:48.662287 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:32:48.662781 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:32:48.668149 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:32:48.668367 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:32:48.668509 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:32:49.435798 [debug] [ThreadPool]: SQL status: OK in 0.77 seconds
[0m23:32:49.451627 [debug] [ThreadPool]: On list_schemas: Close
[0m23:32:49.641563 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_dbt_silver'
[0m23:32:49.655737 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:49.656121 [debug] [ThreadPool]: Using databricks connection "list_None_dbt_silver"
[0m23:32:49.656457 [debug] [ThreadPool]: On list_None_dbt_silver: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_dbt_silver"} */
show tables in `dbt_silver`
  
[0m23:32:49.656752 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:32:50.329764 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m23:32:50.335021 [debug] [ThreadPool]: On list_None_dbt_silver: ROLLBACK
[0m23:32:50.335521 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:32:50.335890 [debug] [ThreadPool]: On list_None_dbt_silver: Close
[0m23:32:50.484295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d66ee0>]}
[0m23:32:50.485538 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:50.486021 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:32:50.487102 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:32:50.487809 [info ] [MainThread]: 
[0m23:32:50.496112 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m23:32:50.496794 [info ] [Thread-1  ]: 1 of 1 START sql table model dbt_silver.silver_cities .......................... [RUN]
[0m23:32:50.497695 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m23:32:50.497987 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m23:32:50.502179 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m23:32:50.502875 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-21 23:32:50.498287 => 2023-02-21 23:32:50.502817
[0m23:32:50.503153 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m23:32:50.549932 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m23:32:50.550302 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:50.550479 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m23:32:50.550648 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `dbt_silver`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id 
limit 5
  
[0m23:32:50.550792 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:33:03.864970 [debug] [Thread-1  ]: SQL status: OK in 13.31 seconds
[0m23:33:04.112748 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-21 23:32:50.503425 => 2023-02-21 23:33:04.112673
[0m23:33:04.113209 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m23:33:04.113482 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:33:04.113712 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m23:33:04.288546 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d7b22c1-7ebb-45c4-bdb6-b8c8ffa241ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152f60820>]}
[0m23:33:04.290042 [info ] [Thread-1  ]: 1 of 1 OK created sql table model dbt_silver.silver_cities ..................... [[32mOK[0m in 13.79s]
[0m23:33:04.292859 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m23:33:04.295241 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:33:04.295765 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:04.296256 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:33:04.486833 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:04.487999 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:04.488634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:33:04.489269 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:04.489706 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:04.490130 [debug] [MainThread]: On master: Close
[0m23:33:04.638594 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:33:04.639688 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m23:33:04.642460 [info ] [MainThread]: 
[0m23:33:04.643415 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 15.98 seconds (15.98s).
[0m23:33:04.644520 [debug] [MainThread]: Command end result
[0m23:33:04.657309 [info ] [MainThread]: 
[0m23:33:04.657847 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:33:04.658362 [info ] [MainThread]: 
[0m23:33:04.658798 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:33:04.659315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152db1f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d66f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152d66520>]}
[0m23:33:04.659709 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:35:21.902133 | ded5eafc-3ede-4f61-8b58-7664fb8cc89b ==============================
[0m23:35:21.902133 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:35:21.903726 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:35:21.904045 [debug] [MainThread]: Tracking: tracking
[0m23:35:21.916562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135f56160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135f56f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135f565e0>]}
[0m23:35:21.958141 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m23:35:21.958464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x135dae430>]}
[0m23:35:22.429017 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m23:35:22.435478 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m23:35:22.437305 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m23:35:22.474769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136a3b0d0>]}
[0m23:35:22.478327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1368494f0>]}
[0m23:35:22.478518 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:35:22.478734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136a1e550>]}
[0m23:35:22.479259 [info ] [MainThread]: 
[0m23:35:22.480117 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:35:22.480569 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:35:22.485833 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:35:22.486062 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:35:22.486192 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:35:23.222967 [debug] [ThreadPool]: SQL status: OK in 0.74 seconds
[0m23:35:23.237583 [debug] [ThreadPool]: On list_schemas: Close
[0m23:35:23.440570 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:35:23.457973 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:23.458345 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:35:23.458627 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:35:23.458888 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:35:24.215954 [debug] [ThreadPool]: SQL status: OK in 0.76 seconds
[0m23:35:24.231466 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:35:24.231968 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:35:24.637274 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:35:24.642211 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:35:24.642777 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:35:24.643193 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:35:24.800287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136a2ceb0>]}
[0m23:35:24.801093 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:24.801546 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:35:24.802496 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:35:24.803196 [info ] [MainThread]: 
[0m23:35:24.809006 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m23:35:24.809642 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities ............................. [RUN]
[0m23:35:24.810731 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m23:35:24.811178 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m23:35:24.813431 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m23:35:24.814026 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-21 23:35:24.811452 => 2023-02-21 23:35:24.813977
[0m23:35:24.814248 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m23:35:24.858626 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m23:35:24.859135 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:24.859326 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m23:35:24.859515 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `airbyte`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id 
limit 5
  
[0m23:35:24.859670 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:35:28.231376 [debug] [Thread-1  ]: SQL status: OK in 3.37 seconds
[0m23:35:28.260117 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-21 23:35:24.814397 => 2023-02-21 23:35:28.260049
[0m23:35:28.260527 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m23:35:28.260795 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:35:28.261027 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m23:35:28.431563 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ded5eafc-3ede-4f61-8b58-7664fb8cc89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c40370>]}
[0m23:35:28.433417 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities ........................ [[32mOK[0m in 3.62s]
[0m23:35:28.437494 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m23:35:28.440166 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:35:28.440821 [debug] [MainThread]: On master: ROLLBACK
[0m23:35:28.441345 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:35:28.626491 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:35:28.627827 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:35:28.628843 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:35:28.629235 [debug] [MainThread]: On master: ROLLBACK
[0m23:35:28.629566 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:35:28.629892 [debug] [MainThread]: On master: Close
[0m23:35:28.795444 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:35:28.796135 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m23:35:28.798219 [info ] [MainThread]: 
[0m23:35:28.798922 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 6.32 seconds (6.32s).
[0m23:35:28.799786 [debug] [MainThread]: Command end result
[0m23:35:28.810162 [info ] [MainThread]: 
[0m23:35:28.810859 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:35:28.811408 [info ] [MainThread]: 
[0m23:35:28.811911 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:35:28.812493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1368e9af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c40100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c40460>]}
[0m23:35:28.812928 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:42:34.246742 | da583ba4-159b-4056-bc91-b3e7001f959f ==============================
[0m23:42:34.246742 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:42:34.248259 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:42:34.248433 [debug] [MainThread]: Tracking: tracking
[0m23:42:34.266574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122216160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122216f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122216880>]}
[0m23:42:34.324465 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m23:42:34.324711 [debug] [MainThread]: Partial parsing: added file: dbx_cities://models/gold_users_cities_join.sql
[0m23:42:34.333128 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:42:34.345119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12237a2e0>]}
[0m23:42:34.348777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103196f70>]}
[0m23:42:34.348983 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:42:34.349196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103921250>]}
[0m23:42:34.349711 [info ] [MainThread]: 
[0m23:42:34.350545 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:42:34.351011 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:42:34.356410 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:42:34.356626 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:42:34.356755 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:42:35.243623 [debug] [ThreadPool]: SQL status: OK in 0.89 seconds
[0m23:42:35.260039 [debug] [ThreadPool]: On list_schemas: Close
[0m23:42:35.434409 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:42:35.448800 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:35.449216 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:42:35.449530 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:42:35.449825 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:42:36.134514 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m23:42:36.150392 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:42:36.150819 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:42:36.567541 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m23:42:36.573556 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:42:36.574201 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:42:36.574634 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:42:36.737489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122418df0>]}
[0m23:42:36.738878 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:36.739473 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:42:36.740666 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:42:36.741385 [info ] [MainThread]: 
[0m23:42:36.748622 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:42:36.749105 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:42:36.750002 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:42:36.750392 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:42:36.754197 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:42:36.755025 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:42:36.750692 => 2023-02-21 23:42:36.754953
[0m23:42:36.755406 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:42:36.781473 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:42:36.781963 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:36.782167 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:42:36.782369 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:42:36.782540 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:42:37.534185 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:42:37.535426 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:42:37.535800 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:569)
	... 19 more

[0m23:42:37.536215 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2Ak\xee\x1c\xdf\xa6\xb3\xb3\x83Q\xb3\xa8\x86'
[0m23:42:37.537030 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:42:36.755661 => 2023-02-21 23:42:37.536886
[0m23:42:37.537491 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:42:37.537860 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:42:37.538213 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:42:37.733370 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:42:37.733793 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da583ba4-159b-4056-bc91-b3e7001f959f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12248b790>]}
[0m23:42:37.734179 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 0.98s]
[0m23:42:37.735424 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:42:37.736420 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:42:37.736660 [debug] [MainThread]: On master: ROLLBACK
[0m23:42:37.736848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:42:37.909773 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:37.910748 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:37.911190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:42:37.911668 [debug] [MainThread]: On master: ROLLBACK
[0m23:42:37.912111 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:37.912547 [debug] [MainThread]: On master: Close
[0m23:42:38.061349 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:42:38.062482 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:42:38.065239 [info ] [MainThread]: 
[0m23:42:38.066004 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.72 seconds (3.72s).
[0m23:42:38.066881 [debug] [MainThread]: Command end result
[0m23:42:38.080015 [info ] [MainThread]: 
[0m23:42:38.080681 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:42:38.081127 [info ] [MainThread]: 
[0m23:42:38.081557 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:42:38.081988 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m23:42:38.082383 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m23:42:38.082653 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:42:38.082930 [info ] [MainThread]: 
[0m23:42:38.083267 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:42:38.083769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12235fe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1031962b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122480a30>]}
[0m23:42:38.084143 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:43:22.731311 | a420e9a4-e1ab-43e2-b68a-5db235ea6f56 ==============================
[0m23:43:22.731311 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:43:22.733040 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:43:22.733342 [debug] [MainThread]: Tracking: tracking
[0m23:43:22.753854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acf160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152acf5e0>]}
[0m23:43:22.807343 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:43:22.807652 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:43:22.816069 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:43:22.827819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152c321f0>]}
[0m23:43:22.831614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a56160>]}
[0m23:43:22.831795 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:43:22.832007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152c12fd0>]}
[0m23:43:22.832532 [info ] [MainThread]: 
[0m23:43:22.833371 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:43:22.833864 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:43:22.839528 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:43:22.839792 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:43:22.839942 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:43:23.417086 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m23:43:23.433122 [debug] [ThreadPool]: On list_schemas: Close
[0m23:43:23.589388 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:43:23.606843 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:23.607221 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:43:23.607529 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:43:23.607789 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:43:24.282955 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m23:43:24.298979 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:43:24.299381 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:43:24.703574 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:43:24.708320 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:43:24.708846 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:43:24.709275 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:43:24.862270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152cdb4f0>]}
[0m23:43:24.863060 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:24.863449 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:43:24.864313 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:43:24.864844 [info ] [MainThread]: 
[0m23:43:24.878034 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:43:24.878459 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:43:24.879097 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:43:24.879308 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:43:24.881095 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:43:24.881523 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:43:24.879450 => 2023-02-21 23:43:24.881482
[0m23:43:24.881686 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:43:24.899962 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:43:24.900451 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:24.900626 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:43:24.900774 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    -- 

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:43:24.900909 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:43:25.714161 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    -- 

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:43:25.715066 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:43:25.715396 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:569)
	... 19 more

[0m23:43:25.715744 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2A\x88\x99\x10\xc8\xb5W\xdd\x90\x04\xc9!\x08'
[0m23:43:25.716510 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:43:24.881807 => 2023-02-21 23:43:25.716373
[0m23:43:25.717044 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:43:25.717407 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:43:25.717724 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:43:25.907425 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:43:25.907856 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a420e9a4-e1ab-43e2-b68a-5db235ea6f56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152e4fbe0>]}
[0m23:43:25.908270 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 1.03s]
[0m23:43:25.909466 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:43:25.910387 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:43:25.910625 [debug] [MainThread]: On master: ROLLBACK
[0m23:43:25.910820 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:43:26.098017 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:43:26.099509 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:43:26.100364 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:43:26.100781 [debug] [MainThread]: On master: ROLLBACK
[0m23:43:26.101115 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:43:26.101440 [debug] [MainThread]: On master: Close
[0m23:43:26.260087 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:43:26.261164 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:43:26.263525 [info ] [MainThread]: 
[0m23:43:26.264343 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.43 seconds (3.43s).
[0m23:43:26.265398 [debug] [MainThread]: Command end result
[0m23:43:26.278560 [info ] [MainThread]: 
[0m23:43:26.279151 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:43:26.279583 [info ] [MainThread]: 
[0m23:43:26.279925 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:43:26.280206 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `snowflake_silver_users` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m23:43:26.280542 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m23:43:26.280949 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 14
[0m23:43:26.281369 [info ] [MainThread]: 
[0m23:43:26.281773 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:43:26.282276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a56040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108827d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a74ee0>]}
[0m23:43:26.282671 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:44:40.118576 | 11ae0a59-e64f-41a3-8f25-ff5e7978cc2c ==============================
[0m23:44:40.118576 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:44:40.123226 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:44:40.123485 [debug] [MainThread]: Tracking: tracking
[0m23:44:40.142252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1269561f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126956f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126956670>]}
[0m23:44:40.198776 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:44:40.199054 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:44:40.206376 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:44:40.218046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bbc2b0>]}
[0m23:44:40.221671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105834280>]}
[0m23:44:40.221859 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:44:40.222067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10644fe50>]}
[0m23:44:40.222594 [info ] [MainThread]: 
[0m23:44:40.223424 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:44:40.223904 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:44:40.229241 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:44:40.229465 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:44:40.229596 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:44:40.881592 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m23:44:40.899177 [debug] [ThreadPool]: On list_schemas: Close
[0m23:44:41.124481 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:44:41.135352 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:41.135675 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:44:41.135942 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:44:41.136192 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:44:41.805808 [debug] [ThreadPool]: SQL status: OK in 0.67 seconds
[0m23:44:41.823055 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:44:41.823457 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:44:42.223681 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:44:42.230505 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:44:42.231097 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:44:42.231520 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:44:42.383239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126c63790>]}
[0m23:44:42.384443 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:42.384980 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:44:42.386149 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:44:42.386851 [info ] [MainThread]: 
[0m23:44:42.394612 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:44:42.395247 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:44:42.396366 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:44:42.396764 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:44:42.399812 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:44:42.401178 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:44:42.397119 => 2023-02-21 23:44:42.401116
[0m23:44:42.401507 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:44:42.427142 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:44:42.429385 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:42.429596 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:44:42.429806 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:44:42.429984 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:44:42.914848 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:44:42.915734 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
^^^
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:44:42.916881 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
^^^
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

DROP TABLE IF EXISTS snowflake_silver_users;
^^^
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:44:42.919614 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2A\xb6\xcd\x15`\x9c\xa6\xd6\xe1\x025\x82\x8b'
[0m23:44:42.921044 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:44:42.401750 => 2023-02-21 23:44:42.920895
[0m23:44:42.921597 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:44:42.922158 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:44:42.922626 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:44:43.122741 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  create or replace view `airbyte`.`gold_users_cities_join`
    
    
    as
      
  
  DROP TABLE IF EXISTS snowflake_silver_users;
  ^^^
  CREATE TABLE snowflake_silver_users
  USING snowflake
  OPTIONS (
      dbtable 'silver_users',
      sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
      sfUser 'jwszolek',
      sfPassword '.8kki-cqP@f2W@7zYY46',
      sfDatabase 'jwszol',
      sfSchema 'public',
      sfWarehouse 'COMPUTE_WH'
  );
  
  select * from snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.ID == dbx.user_id
  
[0m23:44:43.123228 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '11ae0a59-e64f-41a3-8f25-ff5e7978cc2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126cd0fa0>]}
[0m23:44:43.123677 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 0.73s]
[0m23:44:43.124913 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:44:43.125800 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:44:43.126044 [debug] [MainThread]: On master: ROLLBACK
[0m23:44:43.126243 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:44:43.302826 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:44:43.304219 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:44:43.305083 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:44:43.305939 [debug] [MainThread]: On master: ROLLBACK
[0m23:44:43.306743 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:44:43.307557 [debug] [MainThread]: On master: Close
[0m23:44:43.465575 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:44:43.466517 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:44:43.469374 [info ] [MainThread]: 
[0m23:44:43.470352 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.25 seconds (3.25s).
[0m23:44:43.471415 [debug] [MainThread]: Command end result
[0m23:44:43.484600 [info ] [MainThread]: 
[0m23:44:43.485187 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:44:43.485553 [info ] [MainThread]: 
[0m23:44:43.485939 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:44:43.486352 [error] [MainThread]:   
[0m23:44:43.486748 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'DROP': extra input 'DROP'(line 8, pos 0)
[0m23:44:43.487138 [error] [MainThread]:   
[0m23:44:43.487521 [error] [MainThread]:   == SQL ==
[0m23:44:43.487902 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m23:44:43.488281 [error] [MainThread]:   create or replace view `airbyte`.`gold_users_cities_join`
[0m23:44:43.488659 [error] [MainThread]:     
[0m23:44:43.489038 [error] [MainThread]:     
[0m23:44:43.489423 [error] [MainThread]:     as
[0m23:44:43.489805 [error] [MainThread]:       
[0m23:44:43.490189 [error] [MainThread]:   
[0m23:44:43.490614 [error] [MainThread]:   DROP TABLE IF EXISTS snowflake_silver_users;
[0m23:44:43.491037 [error] [MainThread]:   ^^^
[0m23:44:43.491429 [error] [MainThread]:   CREATE TABLE snowflake_silver_users
[0m23:44:43.491798 [error] [MainThread]:   USING snowflake
[0m23:44:43.492150 [error] [MainThread]:   OPTIONS (
[0m23:44:43.492479 [error] [MainThread]:       dbtable 'silver_users',
[0m23:44:43.492816 [error] [MainThread]:       sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m23:44:43.493167 [error] [MainThread]:       sfUser 'jwszolek',
[0m23:44:43.493502 [error] [MainThread]:       sfPassword '.8kki-cqP@f2W@7zYY46',
[0m23:44:43.493843 [error] [MainThread]:       sfDatabase 'jwszol',
[0m23:44:43.494179 [error] [MainThread]:       sfSchema 'public',
[0m23:44:43.494503 [error] [MainThread]:       sfWarehouse 'COMPUTE_WH'
[0m23:44:43.494827 [error] [MainThread]:   );
[0m23:44:43.495153 [error] [MainThread]:   
[0m23:44:43.495480 [error] [MainThread]:   select * from snowflake_silver_users snow
[0m23:44:43.495853 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.ID == dbx.user_id
[0m23:44:43.496206 [error] [MainThread]:   
[0m23:44:43.496554 [info ] [MainThread]: 
[0m23:44:43.496903 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:44:43.497316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1267addf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10590f610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126cc3a90>]}
[0m23:44:43.497619 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:45:18.163813 | 0c6a835f-ddca-4729-a184-8156f2714dbc ==============================
[0m23:45:18.163813 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:45:18.165197 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:45:18.165445 [debug] [MainThread]: Tracking: tracking
[0m23:45:18.183218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a4531f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a453f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a453670>]}
[0m23:45:18.257330 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:45:18.257691 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:45:18.265932 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:45:18.278695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a5b8790>]}
[0m23:45:18.282595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12128fd90>]}
[0m23:45:18.282818 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:45:18.283033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1220bfe50>]}
[0m23:45:18.283607 [info ] [MainThread]: 
[0m23:45:18.284481 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:45:18.285020 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:45:18.290844 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:45:18.291089 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:45:18.291240 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:18.893376 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
[0m23:45:18.917055 [debug] [ThreadPool]: On list_schemas: Close
[0m23:45:19.064114 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:45:19.076196 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:19.076538 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:45:19.076814 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:45:19.077080 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:45:19.788935 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
[0m23:45:19.806553 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:45:19.807050 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:45:20.205985 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m23:45:20.213104 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:45:20.213749 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:45:20.214182 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:45:20.412306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a65f790>]}
[0m23:45:20.413364 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:20.413880 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:45:20.414967 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:45:20.415717 [info ] [MainThread]: 
[0m23:45:20.423525 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:45:20.424184 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:45:20.425060 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:45:20.425321 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:45:20.427760 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:45:20.431105 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:45:20.425505 => 2023-02-21 23:45:20.431055
[0m23:45:20.431334 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:45:20.452457 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:45:20.453205 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:20.453463 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:45:20.453728 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:45:20.453942 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:45:20.926008 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:45:20.928482 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
^^^
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

[0m23:45:20.929050 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
^^^
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
CREATE TABLE snowflake_silver_users
^^^
USING snowflake
OPTIONS (
    dbtable 'silver_users',
    sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
    sfUser 'jwszolek',
    sfPassword '.8kki-cqP@f2W@7zYY46',
    sfDatabase 'jwszol',
    sfSchema 'public',
    sfWarehouse 'COMPUTE_WH'
);

select * from snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:45:20.929445 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2A\xcdw\x1f\xf7\x83Y\x04\xf1f^\xe3\x84'
[0m23:45:20.930293 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:45:20.431449 => 2023-02-21 23:45:20.930148
[0m23:45:20.930765 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:45:20.931138 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:45:20.931468 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:45:21.129951 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  create or replace view `airbyte`.`gold_users_cities_join`
    
    
    as
      
  
  -- DROP TABLE IF EXISTS snowflake_silver_users;
  CREATE TABLE snowflake_silver_users
  ^^^
  USING snowflake
  OPTIONS (
      dbtable 'silver_users',
      sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
      sfUser 'jwszolek',
      sfPassword '.8kki-cqP@f2W@7zYY46',
      sfDatabase 'jwszol',
      sfSchema 'public',
      sfWarehouse 'COMPUTE_WH'
  );
  
  select * from snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.ID == dbx.user_id
  
[0m23:45:21.130362 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c6a835f-ddca-4729-a184-8156f2714dbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a6cdfa0>]}
[0m23:45:21.130687 [error] [Thread-1  ]: 1 of 1 ERROR creating sql view model airbyte.gold_users_cities_join ............ [[31mERROR[0m in 0.71s]
[0m23:45:21.131655 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:45:21.132437 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:45:21.132626 [debug] [MainThread]: On master: ROLLBACK
[0m23:45:21.132779 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:45:21.311724 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:45:21.313401 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:45:21.314174 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:45:21.314631 [debug] [MainThread]: On master: ROLLBACK
[0m23:45:21.315028 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:45:21.315422 [debug] [MainThread]: On master: Close
[0m23:45:21.566590 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:45:21.568108 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:45:21.570654 [info ] [MainThread]: 
[0m23:45:21.571221 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.29 seconds (3.29s).
[0m23:45:21.571839 [debug] [MainThread]: Command end result
[0m23:45:21.580925 [info ] [MainThread]: 
[0m23:45:21.581407 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:45:21.581696 [info ] [MainThread]: 
[0m23:45:21.581977 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:45:21.582286 [error] [MainThread]:   
[0m23:45:21.582629 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE': extra input 'CREATE'(line 9, pos 0)
[0m23:45:21.582963 [error] [MainThread]:   
[0m23:45:21.583304 [error] [MainThread]:   == SQL ==
[0m23:45:21.583644 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m23:45:21.583981 [error] [MainThread]:   create or replace view `airbyte`.`gold_users_cities_join`
[0m23:45:21.584322 [error] [MainThread]:     
[0m23:45:21.584660 [error] [MainThread]:     
[0m23:45:21.585001 [error] [MainThread]:     as
[0m23:45:21.585329 [error] [MainThread]:       
[0m23:45:21.585662 [error] [MainThread]:   
[0m23:45:21.586005 [error] [MainThread]:   -- DROP TABLE IF EXISTS snowflake_silver_users;
[0m23:45:21.586328 [error] [MainThread]:   CREATE TABLE snowflake_silver_users
[0m23:45:21.586668 [error] [MainThread]:   ^^^
[0m23:45:21.586989 [error] [MainThread]:   USING snowflake
[0m23:45:21.587311 [error] [MainThread]:   OPTIONS (
[0m23:45:21.587592 [error] [MainThread]:       dbtable 'silver_users',
[0m23:45:21.587871 [error] [MainThread]:       sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m23:45:21.588149 [error] [MainThread]:       sfUser 'jwszolek',
[0m23:45:21.588427 [error] [MainThread]:       sfPassword '.8kki-cqP@f2W@7zYY46',
[0m23:45:21.588704 [error] [MainThread]:       sfDatabase 'jwszol',
[0m23:45:21.588980 [error] [MainThread]:       sfSchema 'public',
[0m23:45:21.589257 [error] [MainThread]:       sfWarehouse 'COMPUTE_WH'
[0m23:45:21.589534 [error] [MainThread]:   );
[0m23:45:21.589812 [error] [MainThread]:   
[0m23:45:21.590097 [error] [MainThread]:   select * from snowflake_silver_users snow
[0m23:45:21.590391 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.ID == dbx.user_id
[0m23:45:21.590684 [error] [MainThread]:   
[0m23:45:21.590981 [info ] [MainThread]: 
[0m23:45:21.591274 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:45:21.591641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a3d9df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c28e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a6c0a90>]}
[0m23:45:21.591939 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:53:19.578639 | 75433c39-43de-4261-a19a-cb9196637fbb ==============================
[0m23:53:19.578639 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:53:19.580111 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:53:19.580346 [debug] [MainThread]: Tracking: tracking
[0m23:53:19.591039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126a931f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126a93f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126a93670>]}
[0m23:53:19.649667 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:53:19.649949 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:53:19.657303 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:53:19.669092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bf9a30>]}
[0m23:53:19.672993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065c1280>]}
[0m23:53:19.673185 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:53:19.673406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1072bfe50>]}
[0m23:53:19.673966 [info ] [MainThread]: 
[0m23:53:19.674800 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:53:19.675275 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:53:19.681112 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:53:19.681379 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:53:19.681526 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:20.281052 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
[0m23:53:20.296373 [debug] [ThreadPool]: On list_schemas: Close
[0m23:53:20.466786 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:53:20.485705 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:20.486192 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:53:20.486513 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:53:20.486811 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:53:21.133830 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m23:53:21.151304 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:53:21.151715 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:53:21.562260 [debug] [ThreadPool]: SQL status: OK in 0.41 seconds
[0m23:53:21.568448 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:53:21.569084 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:53:21.569539 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:53:21.817006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126da3790>]}
[0m23:53:21.818372 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:21.818897 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:53:21.820007 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:53:21.822600 [info ] [MainThread]: 
[0m23:53:21.830219 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:53:21.830962 [info ] [Thread-1  ]: 1 of 1 START sql view model airbyte.gold_users_cities_join ..................... [RUN]
[0m23:53:21.832125 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:53:21.832538 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:53:21.836003 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:53:21.837051 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:53:21.832848 => 2023-02-21 23:53:21.836976
[0m23:53:21.837423 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:53:21.861030 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:53:21.861496 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:21.861701 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:53:21.861914 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
create or replace view `airbyte`.`gold_users_cities_join`
  
  
  as
    

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users
-- join airbyte.silver_cities dbx on snow.ID == dbx.user_id;

[0m23:53:21.862084 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:53:23.197341 [debug] [Thread-1  ]: SQL status: OK in 1.33 seconds
[0m23:53:23.217883 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:53:21.837680 => 2023-02-21 23:53:23.217804
[0m23:53:23.218363 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:53:23.218676 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:53:23.218951 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:53:23.366315 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75433c39-43de-4261-a19a-cb9196637fbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126f22370>]}
[0m23:53:23.367815 [info ] [Thread-1  ]: 1 of 1 OK created sql view model airbyte.gold_users_cities_join ................ [[32mOK[0m in 1.53s]
[0m23:53:23.371303 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:53:23.375461 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:53:23.376335 [debug] [MainThread]: On master: ROLLBACK
[0m23:53:23.376777 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:53:23.561286 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:53:23.562503 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:53:23.562978 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:53:23.563485 [debug] [MainThread]: On master: ROLLBACK
[0m23:53:23.563921 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:53:23.564366 [debug] [MainThread]: On master: Close
[0m23:53:23.721194 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:53:23.722191 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:53:23.724996 [info ] [MainThread]: 
[0m23:53:23.725891 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.05 seconds (4.05s).
[0m23:53:23.726995 [debug] [MainThread]: Command end result
[0m23:53:23.738780 [info ] [MainThread]: 
[0m23:53:23.739382 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:53:23.739835 [info ] [MainThread]: 
[0m23:53:23.740264 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:53:23.740810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126f30f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126f02a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126bd9100>]}
[0m23:53:23.741192 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:55:07.911025 | 9afe1ebd-d6f4-4b51-98f4-6e5a81c06801 ==============================
[0m23:55:07.911025 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:55:07.912713 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:55:07.912920 [debug] [MainThread]: Tracking: tracking
[0m23:55:07.924453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d5941f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d594f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d594670>]}
[0m23:55:07.988547 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:55:07.988840 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:55:07.996110 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:55:08.007841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6fb430>]}
[0m23:55:08.011627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106beed90>]}
[0m23:55:08.011812 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:55:08.012031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077fce50>]}
[0m23:55:08.012547 [info ] [MainThread]: 
[0m23:55:08.013452 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:55:08.014000 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:55:08.019807 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:55:08.020036 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:55:08.020163 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:55:08.646979 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m23:55:08.665249 [debug] [ThreadPool]: On list_schemas: Close
[0m23:55:08.846610 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:55:08.867437 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:08.867826 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:55:08.868104 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:55:08.868371 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:55:09.480896 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m23:55:09.493383 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:55:09.493654 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:55:10.008072 [debug] [ThreadPool]: SQL status: OK in 0.51 seconds
[0m23:55:10.015342 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:55:10.016158 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:55:10.016615 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:55:10.171191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d923460>]}
[0m23:55:10.172634 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:10.173216 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:55:10.174385 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:55:10.175125 [info ] [MainThread]: 
[0m23:55:10.183019 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:55:10.183723 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:55:10.184955 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:55:10.185289 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:55:10.189229 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:55:10.190242 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:55:10.185600 => 2023-02-21 23:55:10.190190
[0m23:55:10.190473 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:55:10.205104 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:10.205381 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:55:10.205613 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:55:10.205819 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:55:11.241516 [debug] [Thread-1  ]: SQL status: OK in 1.04 seconds
[0m23:55:11.259615 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:55:11.260014 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
drop view if exists `airbyte`.`gold_users_cities_join`
[0m23:55:12.635675 [debug] [Thread-1  ]: SQL status: OK in 1.38 seconds
[0m23:55:12.678321 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:55:12.678885 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:55:12.679122 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users
-- join airbyte.silver_cities dbx on snow.ID == dbx.user_id;
  
[0m23:55:19.438266 [debug] [Thread-1  ]: SQL status: OK in 6.76 seconds
[0m23:55:19.733187 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:55:10.190638 => 2023-02-21 23:55:19.733115
[0m23:55:19.733632 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:55:19.733898 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:55:19.734130 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:55:19.882971 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9afe1ebd-d6f4-4b51-98f4-6e5a81c06801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d923370>]}
[0m23:55:19.884306 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 9.70s]
[0m23:55:19.887406 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:55:19.890492 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:55:19.891174 [debug] [MainThread]: On master: ROLLBACK
[0m23:55:19.891636 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:55:20.084701 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:55:20.085999 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:55:20.086478 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:55:20.086982 [debug] [MainThread]: On master: ROLLBACK
[0m23:55:20.087406 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:55:20.087800 [debug] [MainThread]: On master: Close
[0m23:55:20.238311 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:55:20.239338 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:55:20.242153 [info ] [MainThread]: 
[0m23:55:20.242982 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.23 seconds (12.23s).
[0m23:55:20.243986 [debug] [MainThread]: Command end result
[0m23:55:20.260727 [info ] [MainThread]: 
[0m23:55:20.261345 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:55:20.261786 [info ] [MainThread]: 
[0m23:55:20.262231 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:55:20.262748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d903c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d930f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d923460>]}
[0m23:55:20.263129 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:57:11.132130 | f8e7d765-99dc-41a1-8436-9348a88c44a9 ==============================
[0m23:57:11.132130 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:57:11.133703 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:57:11.133908 [debug] [MainThread]: Tracking: tracking
[0m23:57:11.148929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129fd6190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129fd6fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129fd6610>]}
[0m23:57:11.207153 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:57:11.207438 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:57:11.216073 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:57:11.228002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a23f5b0>]}
[0m23:57:11.231552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129f636a0>]}
[0m23:57:11.231749 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:57:11.231976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103579b50>]}
[0m23:57:11.232496 [info ] [MainThread]: 
[0m23:57:11.233350 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:11.233794 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:57:11.239498 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:57:11.239732 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:57:11.239865 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:11.902815 [debug] [ThreadPool]: SQL status: OK in 0.66 seconds
[0m23:57:11.917152 [debug] [ThreadPool]: On list_schemas: Close
[0m23:57:12.094127 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:57:12.109145 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:12.109555 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:12.109878 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:57:12.110172 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:12.817945 [debug] [ThreadPool]: SQL status: OK in 0.71 seconds
[0m23:57:12.835301 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:12.835755 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:57:13.254265 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m23:57:13.260839 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:57:13.261457 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:57:13.261834 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:57:13.412614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a2e20d0>]}
[0m23:57:13.414035 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:13.414496 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:13.415543 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:57:13.416085 [info ] [MainThread]: 
[0m23:57:13.424605 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:57:13.425245 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:57:13.426356 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:57:13.426749 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:57:13.430357 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:13.431350 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:57:13.427054 => 2023-02-21 23:57:13.431281
[0m23:57:13.431695 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:57:13.446167 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:13.446444 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:13.446671 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:57:13.446865 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:57:14.146539 [debug] [Thread-1  ]: SQL status: OK in 0.7 seconds
[0m23:57:14.192325 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:14.192832 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:14.193043 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;
  
[0m23:57:14.897205 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.ID == dbx.user_id;
  
[0m23:57:14.898668 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:14.899128 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:57:14.899554 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2Cv\xc6\x13\xb3\x9c\xff\xc2\x88\xfe\x10\x92\xec'
[0m23:57:14.900468 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:57:13.431946 => 2023-02-21 23:57:14.900313
[0m23:57:14.901002 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:57:14.901410 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:57:14.901806 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:57:15.097131 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:15.097562 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8e7d765-99dc-41a1-8436-9348a88c44a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a363370>]}
[0m23:57:15.097941 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.67s]
[0m23:57:15.099508 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:57:15.100787 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:15.101054 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:15.101259 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:15.276962 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:15.278646 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:15.279214 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:15.279726 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:15.280172 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:15.280603 [debug] [MainThread]: On master: Close
[0m23:57:15.426519 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:15.427037 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:57:15.429328 [info ] [MainThread]: 
[0m23:57:15.430338 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.20 seconds (4.20s).
[0m23:57:15.431638 [debug] [MainThread]: Command end result
[0m23:57:15.442839 [info ] [MainThread]: 
[0m23:57:15.443611 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:57:15.444169 [info ] [MainThread]: 
[0m23:57:15.444550 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:57:15.444831 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:15.445079 [info ] [MainThread]: 
[0m23:57:15.445431 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:57:15.445766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x129f636a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a342bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12a21ec40>]}
[0m23:57:15.446008 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:57:50.685111 | 8a978594-9b15-49c3-9678-9efe2fcd5663 ==============================
[0m23:57:50.685111 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:57:50.686813 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:57:50.687015 [debug] [MainThread]: Tracking: tracking
[0m23:57:50.698879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb12190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb12fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb12610>]}
[0m23:57:50.754896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:57:50.755176 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:57:50.763844 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:57:50.775903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cd7e7f0>]}
[0m23:57:50.779615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca9f6a0>]}
[0m23:57:50.779802 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:57:50.780017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039f1b50>]}
[0m23:57:50.780549 [info ] [MainThread]: 
[0m23:57:50.781396 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:50.781876 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:57:50.787325 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:57:50.787570 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:57:50.787714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:57:51.441480 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m23:57:51.456170 [debug] [ThreadPool]: On list_schemas: Close
[0m23:57:51.686797 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:57:51.701007 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:51.701393 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:51.701669 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:57:51.701922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:57:52.315924 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m23:57:52.330480 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:57:52.330837 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:57:52.754348 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m23:57:52.758145 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:57:52.758439 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:57:52.758651 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:57:52.931267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9220d0>]}
[0m23:57:52.932327 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:52.932822 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:52.933925 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:57:52.934692 [info ] [MainThread]: 
[0m23:57:52.941641 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:57:52.942259 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:57:52.943365 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:57:52.943769 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:57:52.947198 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:52.948520 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:57:52.944080 => 2023-02-21 23:57:52.948453
[0m23:57:52.948850 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:57:52.963422 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:52.963685 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:52.963915 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:57:52.964108 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:57:53.585019 [debug] [Thread-1  ]: SQL status: OK in 0.62 seconds
[0m23:57:53.634213 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:57:53.635675 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:57:53.635890 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:57:54.218335 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select * from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:57:54.219596 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:54.220107 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:57:54.220521 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\x8eF\x1f\x84\xa2\rC\x15\x7f]\xf4!'
[0m23:57:54.221481 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:57:52.949097 => 2023-02-21 23:57:54.221316
[0m23:57:54.222060 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:57:54.222506 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:57:54.222910 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:57:54.412609 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:54.412960 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a978594-9b15-49c3-9678-9efe2fcd5663', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c9a3370>]}
[0m23:57:54.413306 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.47s]
[0m23:57:54.414476 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:57:54.415480 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:57:54.415721 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:54.415963 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:54.592239 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:54.593285 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:54.593797 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:54.594303 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:54.594747 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:54.595181 [debug] [MainThread]: On master: Close
[0m23:57:54.755197 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:54.756207 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:57:54.759000 [info ] [MainThread]: 
[0m23:57:54.759786 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.98 seconds (3.98s).
[0m23:57:54.760727 [debug] [MainThread]: Command end result
[0m23:57:54.772390 [info ] [MainThread]: 
[0m23:57:54.773023 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:57:54.773472 [info ] [MainThread]: 
[0m23:57:54.773845 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:57:54.774294 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:57:54.774799 [info ] [MainThread]: 
[0m23:57:54.775281 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:57:54.775795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca9f6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12c983bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cd5dc40>]}
[0m23:57:54.776175 [debug] [MainThread]: Flushing usage events


============================== 2023-02-21 23:59:39.691600 | f1f0f95f-7296-4e65-989d-9517a2efbab0 ==============================
[0m23:59:39.691600 [info ] [MainThread]: Running with dbt=1.4.1
[0m23:59:39.693604 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:59:39.694355 [debug] [MainThread]: Tracking: tracking
[0m23:59:39.707470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126616160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126616f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1266165e0>]}
[0m23:59:39.761444 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:59:39.761715 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m23:59:39.769046 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m23:59:39.780823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12677fd60>]}
[0m23:59:39.784577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10600be80>]}
[0m23:59:39.784765 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:59:39.784980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1053ca670>]}
[0m23:59:39.785511 [info ] [MainThread]: 
[0m23:59:39.786358 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:59:39.786833 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m23:59:39.792171 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m23:59:39.792412 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m23:59:39.792570 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:59:40.486522 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
[0m23:59:40.500643 [debug] [ThreadPool]: On list_schemas: Close
[0m23:59:40.656428 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m23:59:40.673781 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:40.674124 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:59:40.674412 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m23:59:40.674713 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m23:59:41.284888 [debug] [ThreadPool]: SQL status: OK in 0.61 seconds
[0m23:59:41.300412 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m23:59:41.300782 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m23:59:41.739698 [debug] [ThreadPool]: SQL status: OK in 0.44 seconds
[0m23:59:41.746082 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m23:59:41.746713 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m23:59:41.747143 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m23:59:41.902161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268234f0>]}
[0m23:59:41.903430 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:41.903929 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:59:41.905089 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:59:41.905849 [info ] [MainThread]: 
[0m23:59:41.912864 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m23:59:41.913491 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m23:59:41.914569 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m23:59:41.914960 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m23:59:41.918435 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m23:59:41.919859 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-21 23:59:41.915272 => 2023-02-21 23:59:41.919783
[0m23:59:41.920209 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m23:59:41.935034 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:41.935333 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:59:41.935582 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m23:59:41.935781 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m23:59:42.567258 [debug] [Thread-1  ]: SQL status: OK in 0.63 seconds
[0m23:59:42.617066 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m23:59:42.617522 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m23:59:42.617724 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.*, dbx.* from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:59:43.269839 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.*, dbx.* from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m23:59:43.270757 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:59:43.271254 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m23:59:43.271689 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\xcf<\x10\xaf\x9c\xf8A\t\xf1h\xf0\xb1'
[0m23:59:43.272588 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-21 23:59:41.920462 => 2023-02-21 23:59:43.272417
[0m23:59:43.273107 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m23:59:43.273525 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m23:59:43.273929 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m23:59:43.462335 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:59:43.462774 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f0f95f-7296-4e65-989d-9517a2efbab0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a2370>]}
[0m23:59:43.463191 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.55s]
[0m23:59:43.464539 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m23:59:43.465535 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:59:43.465753 [debug] [MainThread]: On master: ROLLBACK
[0m23:59:43.465950 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:59:43.642518 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:59:43.643359 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:59:43.643821 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:59:43.644307 [debug] [MainThread]: On master: ROLLBACK
[0m23:59:43.644772 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:59:43.645217 [debug] [MainThread]: On master: Close
[0m23:59:43.800513 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:59:43.801653 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m23:59:43.803991 [info ] [MainThread]: 
[0m23:59:43.804754 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.02 seconds (4.02s).
[0m23:59:43.805672 [debug] [MainThread]: Command end result
[0m23:59:43.817831 [info ] [MainThread]: 
[0m23:59:43.818463 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:59:43.818990 [info ] [MainThread]: 
[0m23:59:43.819383 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m23:59:43.819732 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`, `name`.
[0m23:59:43.820107 [info ] [MainThread]: 
[0m23:59:43.820523 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:59:43.821024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1264965b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268a2370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1268b0ee0>]}
[0m23:59:43.821381 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:00:01.941349 | 0e10869a-7fd6-49d4-b965-8278341dd1ef ==============================
[0m00:00:01.941349 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:00:01.947851 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:00:01.948204 [debug] [MainThread]: Tracking: tracking
[0m00:00:01.958487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150856160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150856f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150856880>]}
[0m00:00:02.018108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:00:02.018387 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:00:02.025570 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:00:02.037397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1509bbb80>]}
[0m00:00:02.041167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1257b2310>]}
[0m00:00:02.041355 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:00:02.041569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064ce550>]}
[0m00:00:02.042082 [info ] [MainThread]: 
[0m00:00:02.042909 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:02.043394 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:00:02.049037 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:00:02.049282 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:00:02.049427 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:02.699004 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m00:00:02.711538 [debug] [ThreadPool]: On list_schemas: Close
[0m00:00:02.875858 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:00:02.892596 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:02.892964 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:00:02.893255 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:00:02.893513 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:00:03.520081 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m00:00:03.538111 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:00:03.538472 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:00:04.031594 [debug] [ThreadPool]: SQL status: OK in 0.49 seconds
[0m00:00:04.038267 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:00:04.038964 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:00:04.039403 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:00:04.197857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150b62490>]}
[0m00:00:04.199067 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:04.199685 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:04.200897 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:00:04.201737 [info ] [MainThread]: 
[0m00:00:04.208990 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:00:04.209493 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:00:04.210545 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:00:04.210946 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:00:04.214501 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:00:04.215180 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:00:04.211251 => 2023-02-22 00:00:04.215104
[0m00:00:04.215578 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:00:04.230948 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:04.231223 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:00:04.231451 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:00:04.231650 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:00:04.952410 [debug] [Thread-1  ]: SQL status: OK in 0.72 seconds
[0m00:00:05.003673 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:00:05.004129 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:00:05.004336 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id, dbx.id from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:00:05.567615 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id, dbx.id from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:00:05.568991 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
[0m00:00:05.569745 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1233] org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.foundDuplicateColumnError(QueryCompilationErrors.scala:2483)
	at org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:123)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:344)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2.applyOrElse(rules.scala:183)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:171)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:165)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:183)
	at org.apache.spark.sql.execution.datasources.PreprocessTableCreation.apply(rules.scala:179)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:218)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:218)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:277)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:277)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:277)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:194)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:353)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:253)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:346)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:153)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:326)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:325)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:168)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:358)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:794)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:358)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:144)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:477)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m00:00:05.570115 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\xdc\x95\x1b\x9b\x99\xb1%\xe5\n\x00\x02\x15'
[0m00:00:05.570933 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:00:04.215859 => 2023-02-22 00:00:05.570770
[0m00:00:05.571420 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:00:05.571778 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:00:05.572129 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:00:05.770620 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
[0m00:00:05.771126 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e10869a-7fd6-49d4-b965-8278341dd1ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150be3370>]}
[0m00:00:05.771585 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.56s]
[0m00:00:05.772924 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:00:05.773920 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:05.774170 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:05.774378 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:00:05.951745 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:05.952730 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:05.953179 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:05.953605 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:05.953963 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:05.954306 [debug] [MainThread]: On master: Close
[0m00:00:06.104460 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:00:06.105587 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:00:06.108474 [info ] [MainThread]: 
[0m00:00:06.109251 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.07 seconds (4.07s).
[0m00:00:06.110179 [debug] [MainThread]: Command end result
[0m00:00:06.121896 [info ] [MainThread]: 
[0m00:00:06.122596 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:00:06.123134 [info ] [MainThread]: 
[0m00:00:06.123545 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m00:00:06.123911 [error] [MainThread]:   Found duplicate column(s) in the table definition of airbyte.gold_users_cities_join: `id`.
[0m00:00:06.124334 [info ] [MainThread]: 
[0m00:00:06.124759 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:00:06.125273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150c53940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1257b2250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150bc2b20>]}
[0m00:00:06.125674 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:00:58.709033 | 4073598b-7921-40c4-8b10-e826d6b1d07e ==============================
[0m00:00:58.709033 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:00:58.711921 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:00:58.712113 [debug] [MainThread]: Tracking: tracking
[0m00:00:58.731926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ad7160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ad7f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ad75e0>]}
[0m00:00:58.791510 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:00:58.791787 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:00:58.799110 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:00:58.810783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130e3a400>]}
[0m00:00:58.814401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10850fe80>]}
[0m00:00:58.814583 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:00:58.814811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078ce670>]}
[0m00:00:58.815326 [info ] [MainThread]: 
[0m00:00:58.816150 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:58.816618 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:00:58.822060 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:00:58.822320 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:00:58.822454 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:59.541798 [debug] [ThreadPool]: SQL status: OK in 0.72 seconds
[0m00:00:59.557850 [debug] [ThreadPool]: On list_schemas: Close
[0m00:00:59.802362 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:00:59.819496 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:59.819801 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:00:59.820016 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:00:59.820218 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:00.453818 [debug] [ThreadPool]: SQL status: OK in 0.63 seconds
[0m00:01:00.470997 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:01:00.471343 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:01:00.965020 [debug] [ThreadPool]: SQL status: OK in 0.49 seconds
[0m00:01:00.971270 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:01:00.971896 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:01:00.972321 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:01:01.135905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130ee24f0>]}
[0m00:01:01.137122 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:01.137618 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:01.138799 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:01:01.139507 [info ] [MainThread]: 
[0m00:01:01.146341 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:01:01.146957 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:01:01.147977 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:01:01.148266 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:01:01.151709 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.152655 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:01:01.148491 => 2023-02-22 00:01:01.152596
[0m00:01:01.152986 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:01:01.164718 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:01.164942 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.165100 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:01:01.165237 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:01:01.844049 [debug] [Thread-1  ]: SQL status: OK in 0.68 seconds
[0m00:01:01.895508 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.896010 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:01.896237 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:02.144662 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:02.145430 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
------------------^^^
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

[0m00:01:02.145851 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
------------------^^^
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as 'snow_id', 
------------------^^^
       dbx.id as 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m00:01:02.146243 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2C\xfe~\x12\xc0\xadVdo(\x98,\x1f'
[0m00:01:02.147033 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:01:01.153200 => 2023-02-22 00:01:02.146920
[0m00:01:02.147560 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:01:02.147970 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:01:02.148368 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:01:02.335261 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  
    
      
          create or replace table `airbyte`.`gold_users_cities_join`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  -- DROP TABLE IF EXISTS snowflake_silver_users;
  -- CREATE TABLE snowflake_silver_users
  -- USING snowflake
  -- OPTIONS (
  --     dbtable 'silver_users',
  --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
  --     sfUser 'jwszolek',
  --     sfPassword '.8kki-cqP@f2W@7zYY46',
  --     sfDatabase 'jwszol',
  --     sfSchema 'public',
  --     sfWarehouse 'COMPUTE_WH'
  -- );
  
  select snow.id as 'snow_id', 
  ------------------^^^
         dbx.id as 'dbx_id' 
  from hive_metastore.default.snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.id == dbx.user_id
  
[0m00:01:02.335696 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4073598b-7921-40c4-8b10-e826d6b1d07e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130f63370>]}
[0m00:01:02.336071 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.19s]
[0m00:01:02.337304 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:01:02.338342 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:01:02.338594 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:02.338802 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:01:02.495238 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:02.496240 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:02.496694 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:02.497171 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:02.497610 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:02.498042 [debug] [MainThread]: On master: Close
[0m00:01:02.662725 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:01:02.664161 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:01:02.666227 [info ] [MainThread]: 
[0m00:01:02.666914 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.85 seconds (3.85s).
[0m00:01:02.667601 [debug] [MainThread]: Command end result
[0m00:01:02.677143 [info ] [MainThread]: 
[0m00:01:02.677636 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:01:02.678053 [info ] [MainThread]: 
[0m00:01:02.678509 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m00:01:02.678889 [error] [MainThread]:   
[0m00:01:02.679267 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 18)
[0m00:01:02.679666 [error] [MainThread]:   
[0m00:01:02.680052 [error] [MainThread]:   == SQL ==
[0m00:01:02.680440 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m00:01:02.680837 [error] [MainThread]:   
[0m00:01:02.681218 [error] [MainThread]:     
[0m00:01:02.681600 [error] [MainThread]:       
[0m00:01:02.681960 [error] [MainThread]:           create or replace table `airbyte`.`gold_users_cities_join`
[0m00:01:02.682288 [error] [MainThread]:         
[0m00:01:02.682615 [error] [MainThread]:         
[0m00:01:02.682940 [error] [MainThread]:       using delta
[0m00:01:02.683272 [error] [MainThread]:         
[0m00:01:02.683618 [error] [MainThread]:         
[0m00:01:02.683916 [error] [MainThread]:         
[0m00:01:02.684166 [error] [MainThread]:         
[0m00:01:02.684437 [error] [MainThread]:         
[0m00:01:02.684761 [error] [MainThread]:         
[0m00:01:02.685088 [error] [MainThread]:         as
[0m00:01:02.685424 [error] [MainThread]:         
[0m00:01:02.685773 [error] [MainThread]:   
[0m00:01:02.686101 [error] [MainThread]:   -- DROP TABLE IF EXISTS snowflake_silver_users;
[0m00:01:02.686431 [error] [MainThread]:   -- CREATE TABLE snowflake_silver_users
[0m00:01:02.686761 [error] [MainThread]:   -- USING snowflake
[0m00:01:02.687087 [error] [MainThread]:   -- OPTIONS (
[0m00:01:02.687411 [error] [MainThread]:   --     dbtable 'silver_users',
[0m00:01:02.687743 [error] [MainThread]:   --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m00:01:02.688079 [error] [MainThread]:   --     sfUser 'jwszolek',
[0m00:01:02.688416 [error] [MainThread]:   --     sfPassword '.8kki-cqP@f2W@7zYY46',
[0m00:01:02.688743 [error] [MainThread]:   --     sfDatabase 'jwszol',
[0m00:01:02.689086 [error] [MainThread]:   --     sfSchema 'public',
[0m00:01:02.689404 [error] [MainThread]:   --     sfWarehouse 'COMPUTE_WH'
[0m00:01:02.689726 [error] [MainThread]:   -- );
[0m00:01:02.690002 [error] [MainThread]:   
[0m00:01:02.690279 [error] [MainThread]:   select snow.id as 'snow_id', 
[0m00:01:02.690557 [error] [MainThread]:   ------------------^^^
[0m00:01:02.690834 [error] [MainThread]:          dbx.id as 'dbx_id' 
[0m00:01:02.691131 [error] [MainThread]:   from hive_metastore.default.snowflake_silver_users snow
[0m00:01:02.691411 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.id == dbx.user_id
[0m00:01:02.691690 [error] [MainThread]:   
[0m00:01:02.691983 [info ] [MainThread]: 
[0m00:01:02.692283 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:01:02.692641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130f512b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130a5e5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x130f70ee0>]}
[0m00:01:02.692901 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:01:20.027768 | 25a5f8b4-0b85-4a5a-b0f3-979f954b5979 ==============================
[0m00:01:20.027768 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:01:20.029314 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:01:20.029505 [debug] [MainThread]: Tracking: tracking
[0m00:01:20.049970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271cf190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271cffa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271cf610>]}
[0m00:01:20.103116 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:01:20.103392 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:01:20.112392 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:01:20.125033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12743a310>]}
[0m00:01:20.128611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12715b6a0>]}
[0m00:01:20.128797 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:01:20.129053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071b4b50>]}
[0m00:01:20.129613 [info ] [MainThread]: 
[0m00:01:20.130627 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:01:20.131106 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:01:20.136770 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:01:20.137002 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:01:20.137139 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:20.736069 [debug] [ThreadPool]: SQL status: OK in 0.6 seconds
[0m00:01:20.749552 [debug] [ThreadPool]: On list_schemas: Close
[0m00:01:20.902165 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:01:20.919413 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:20.919806 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:01:20.920126 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:01:20.920422 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:01:21.499671 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m00:01:21.514754 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:01:21.515137 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:01:21.915204 [debug] [ThreadPool]: SQL status: OK in 0.4 seconds
[0m00:01:21.922988 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:01:21.923643 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:01:21.924076 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:01:22.075031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1274e7f40>]}
[0m00:01:22.076511 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:22.077020 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:22.078143 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:01:22.078806 [info ] [MainThread]: 
[0m00:01:22.085903 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:01:22.086519 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:01:22.087330 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:01:22.087613 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:01:22.091272 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.091992 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:01:22.087877 => 2023-02-22 00:01:22.091930
[0m00:01:22.092282 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:01:22.106976 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:22.107244 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.107472 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:01:22.107678 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:01:22.750039 [debug] [Thread-1  ]: SQL status: OK in 0.64 seconds
[0m00:01:22.801636 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.802154 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:01:22.802380 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:23.039713 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:01:23.040236 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
---------------^^^
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

[0m00:01:23.040569 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
---------------^^^
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:585)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:484)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:353)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:149)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:331)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:365)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id 'snow_id', 
---------------^^^
       dbx.id 'dbx_id' 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:334)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:165)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:92)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:110)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:463)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:319)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:462)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1003)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:446)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:460)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:519)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:697)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:60)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:519)
	... 19 more

[0m00:01:23.040893 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xed\xb2D\n\xf2\x15\x13\x83\x8aD\xb7\xfe\x85y\xb2'
[0m00:01:23.041480 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:01:22.092487 => 2023-02-22 00:01:23.041398
[0m00:01:23.041826 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:01:23.042114 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:01:23.042395 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:01:23.239831 [debug] [Thread-1  ]: Runtime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
  
    
      
          create or replace table `airbyte`.`gold_users_cities_join`
        
        
      using delta
        
        
        
        
        
        
        as
        
  
  -- DROP TABLE IF EXISTS snowflake_silver_users;
  -- CREATE TABLE snowflake_silver_users
  -- USING snowflake
  -- OPTIONS (
  --     dbtable 'silver_users',
  --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
  --     sfUser 'jwszolek',
  --     sfPassword '.8kki-cqP@f2W@7zYY46',
  --     sfDatabase 'jwszol',
  --     sfSchema 'public',
  --     sfWarehouse 'COMPUTE_WH'
  -- );
  
  select snow.id 'snow_id', 
  ---------------^^^
         dbx.id 'dbx_id' 
  from hive_metastore.default.snowflake_silver_users snow
  join airbyte.silver_cities dbx on snow.id == dbx.user_id
  
[0m00:01:23.240242 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25a5f8b4-0b85-4a5a-b0f3-979f954b5979', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127562370>]}
[0m00:01:23.240605 [error] [Thread-1  ]: 1 of 1 ERROR creating sql table model airbyte.gold_users_cities_join ........... [[31mERROR[0m in 1.15s]
[0m00:01:23.241819 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:01:23.242813 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:01:23.243062 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:23.243260 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:01:23.425277 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:23.426418 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:23.426886 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:23.427378 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:23.427799 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:23.428172 [debug] [MainThread]: On master: Close
[0m00:01:23.606621 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:01:23.607183 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:01:23.610496 [info ] [MainThread]: 
[0m00:01:23.611217 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.48 seconds (3.48s).
[0m00:01:23.611971 [debug] [MainThread]: Command end result
[0m00:01:23.625177 [info ] [MainThread]: 
[0m00:01:23.625696 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:01:23.626058 [info ] [MainThread]: 
[0m00:01:23.626828 [error] [MainThread]: [33mRuntime Error in model gold_users_cities_join (models/gold_users_cities_join.sql)[0m
[0m00:01:23.627370 [error] [MainThread]:   
[0m00:01:23.627934 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near ''snow_id''(line 31, pos 15)
[0m00:01:23.628444 [error] [MainThread]:   
[0m00:01:23.628901 [error] [MainThread]:   == SQL ==
[0m00:01:23.629392 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */
[0m00:01:23.629919 [error] [MainThread]:   
[0m00:01:23.630479 [error] [MainThread]:     
[0m00:01:23.630926 [error] [MainThread]:       
[0m00:01:23.631234 [error] [MainThread]:           create or replace table `airbyte`.`gold_users_cities_join`
[0m00:01:23.631721 [error] [MainThread]:         
[0m00:01:23.632129 [error] [MainThread]:         
[0m00:01:23.632836 [error] [MainThread]:       using delta
[0m00:01:23.633494 [error] [MainThread]:         
[0m00:01:23.634321 [error] [MainThread]:         
[0m00:01:23.634961 [error] [MainThread]:         
[0m00:01:23.635710 [error] [MainThread]:         
[0m00:01:23.636314 [error] [MainThread]:         
[0m00:01:23.636871 [error] [MainThread]:         
[0m00:01:23.637213 [error] [MainThread]:         as
[0m00:01:23.637786 [error] [MainThread]:         
[0m00:01:23.638545 [error] [MainThread]:   
[0m00:01:23.639211 [error] [MainThread]:   -- DROP TABLE IF EXISTS snowflake_silver_users;
[0m00:01:23.639678 [error] [MainThread]:   -- CREATE TABLE snowflake_silver_users
[0m00:01:23.640131 [error] [MainThread]:   -- USING snowflake
[0m00:01:23.640756 [error] [MainThread]:   -- OPTIONS (
[0m00:01:23.641277 [error] [MainThread]:   --     dbtable 'silver_users',
[0m00:01:23.641784 [error] [MainThread]:   --     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
[0m00:01:23.642387 [error] [MainThread]:   --     sfUser 'jwszolek',
[0m00:01:23.642852 [error] [MainThread]:   --     sfPassword '.8kki-cqP@f2W@7zYY46',
[0m00:01:23.643305 [error] [MainThread]:   --     sfDatabase 'jwszol',
[0m00:01:23.643751 [error] [MainThread]:   --     sfSchema 'public',
[0m00:01:23.644196 [error] [MainThread]:   --     sfWarehouse 'COMPUTE_WH'
[0m00:01:23.644640 [error] [MainThread]:   -- );
[0m00:01:23.645078 [error] [MainThread]:   
[0m00:01:23.645516 [error] [MainThread]:   select snow.id 'snow_id', 
[0m00:01:23.645975 [error] [MainThread]:   ---------------^^^
[0m00:01:23.646423 [error] [MainThread]:          dbx.id 'dbx_id' 
[0m00:01:23.646893 [error] [MainThread]:   from hive_metastore.default.snowflake_silver_users snow
[0m00:01:23.647388 [error] [MainThread]:   join airbyte.silver_cities dbx on snow.id == dbx.user_id
[0m00:01:23.647660 [error] [MainThread]:   
[0m00:01:23.647972 [info ] [MainThread]: 
[0m00:01:23.648621 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m00:01:23.649314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12715b6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127542bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12741dc40>]}
[0m00:01:23.649663 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:02:11.031137 | 3cb1975b-c53a-40ea-a2db-64bc13dc9442 ==============================
[0m00:02:11.031137 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:02:11.032773 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:02:11.032968 [debug] [MainThread]: Tracking: tracking
[0m00:02:11.044197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1d71c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1d7fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e1d7640>]}
[0m00:02:11.097205 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:02:11.097488 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:02:11.104809 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:02:11.116668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e33fac0>]}
[0m00:02:11.120444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107923af0>]}
[0m00:02:11.120633 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:02:11.120848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107935df0>]}
[0m00:02:11.121412 [info ] [MainThread]: 
[0m00:02:11.122256 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:02:11.122741 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:02:11.128172 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:02:11.128406 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:02:11.128536 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:11.806900 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m00:02:11.821541 [debug] [ThreadPool]: On list_schemas: Close
[0m00:02:11.995228 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:02:12.013937 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:12.014398 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:02:12.014720 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:02:12.015030 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:02:12.596178 [debug] [ThreadPool]: SQL status: OK in 0.58 seconds
[0m00:02:12.613803 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:02:12.614201 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:02:13.083690 [debug] [ThreadPool]: SQL status: OK in 0.47 seconds
[0m00:02:13.088897 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:02:13.089346 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:02:13.089612 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:02:13.263271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e3e4070>]}
[0m00:02:13.264370 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:13.264856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:02:13.265971 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:02:13.266704 [info ] [MainThread]: 
[0m00:02:13.274053 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:02:13.274638 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:02:13.275682 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:02:13.276005 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:02:13.279434 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.280566 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:02:13.276220 => 2023-02-22 00:02:13.280498
[0m00:02:13.280925 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:02:13.295783 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:13.296040 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.296262 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:02:13.296457 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:02:13.940891 [debug] [Thread-1  ]: SQL status: OK in 0.64 seconds
[0m00:02:13.992957 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.994315 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:02:13.994536 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as snow_id, 
       dbx.id as dbx_id 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:02:29.766381 [debug] [Thread-1  ]: SQL status: OK in 15.77 seconds
[0m00:02:30.063628 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:02:13.281202 => 2023-02-22 00:02:30.063512
[0m00:02:30.064166 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:02:30.064481 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:02:30.064725 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:02:30.217533 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cb1975b-c53a-40ea-a2db-64bc13dc9442', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e462370>]}
[0m00:02:30.219258 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 16.94s]
[0m00:02:30.222798 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:02:30.226436 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:02:30.227059 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:30.227487 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:02:30.414895 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:30.416162 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:30.416634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:02:30.417140 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:30.417586 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:30.418020 [debug] [MainThread]: On master: Close
[0m00:02:30.568441 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:02:30.569408 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:02:30.571811 [info ] [MainThread]: 
[0m00:02:30.572832 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 19.45 seconds (19.45s).
[0m00:02:30.573986 [debug] [MainThread]: Command end result
[0m00:02:30.588584 [info ] [MainThread]: 
[0m00:02:30.589177 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:02:30.589632 [info ] [MainThread]: 
[0m00:02:30.590090 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:02:30.590668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4b6190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4b6d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4b62b0>]}
[0m00:02:30.591054 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:03:53.611512 | f7562c22-0376-453d-9ee4-16759332ce4a ==============================
[0m00:03:53.611512 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:03:53.613196 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:03:53.613366 [debug] [MainThread]: Tracking: tracking
[0m00:03:53.625016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124757190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124757fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124757610>]}
[0m00:03:53.688555 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:03:53.688736 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:03:53.692192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1248dbf40>]}
[0m00:03:53.696400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124820c10>]}
[0m00:03:53.696588 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:03:53.696800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c11b50>]}
[0m00:03:53.697351 [info ] [MainThread]: 
[0m00:03:53.698268 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:03:53.698795 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:03:53.704689 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:03:53.704942 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:03:53.705081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:03:54.347647 [debug] [ThreadPool]: SQL status: OK in 0.64 seconds
[0m00:03:54.364292 [debug] [ThreadPool]: On list_schemas: Close
[0m00:03:54.536441 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:03:54.560785 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:54.561316 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:03:54.561709 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:03:54.561975 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:03:55.252796 [debug] [ThreadPool]: SQL status: OK in 0.69 seconds
[0m00:03:55.273719 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:03:55.274457 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:03:55.843744 [debug] [ThreadPool]: SQL status: OK in 0.57 seconds
[0m00:03:55.853459 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:03:55.854246 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:03:55.854774 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:03:56.007526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124820e20>]}
[0m00:03:56.009276 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:56.010005 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:03:56.011691 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:03:56.012772 [info ] [MainThread]: 
[0m00:03:56.021053 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:03:56.021928 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:03:56.023154 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:03:56.023509 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:03:56.027836 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:03:56.028868 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:03:56.023743 => 2023-02-22 00:03:56.028803
[0m00:03:56.029174 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:03:56.079967 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:03:56.082450 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:03:56.082641 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:03:56.082825 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as snow_id, 
       dbx.id as dbx_id 
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:03:56.082971 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:04:02.793026 [debug] [Thread-1  ]: SQL status: OK in 6.71 seconds
[0m00:04:03.072876 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:03:56.029408 => 2023-02-22 00:04:03.072801
[0m00:04:03.073324 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:04:03.073592 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.073837 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:04:03.249098 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7562c22-0376-453d-9ee4-16759332ce4a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249694c0>]}
[0m00:04:03.250383 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 7.23s]
[0m00:04:03.254521 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:04:03.257377 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:04:03.257944 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:03.258381 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:04:03.451181 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.452766 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:03.453648 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:04:03.454563 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:03.455085 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:03.455484 [debug] [MainThread]: On master: Close
[0m00:04:03.598085 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:04:03.598778 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:04:03.601110 [info ] [MainThread]: 
[0m00:04:03.601872 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.90 seconds (9.90s).
[0m00:04:03.602777 [debug] [MainThread]: Command end result
[0m00:04:03.615419 [info ] [MainThread]: 
[0m00:04:03.615979 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:04:03.616420 [info ] [MainThread]: 
[0m00:04:03.616838 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:04:03.617407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249659a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124999160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124a03790>]}
[0m00:04:03.617770 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:06:09.342524 | 511883b0-2558-4d11-b233-5f3bfc54e695 ==============================
[0m00:06:09.342524 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:06:09.344249 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:06:09.344494 [debug] [MainThread]: Tracking: tracking
[0m00:06:09.356349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba931f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba93f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba93670>]}
[0m00:06:09.418635 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:06:09.418908 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:06:09.427433 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:06:09.439348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcfaa90>]}
[0m00:06:09.442927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10501bd90>]}
[0m00:06:09.443109 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:06:09.443329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105c27e50>]}
[0m00:06:09.443865 [info ] [MainThread]: 
[0m00:06:09.444734 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:06:09.445250 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:06:09.450955 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:06:09.451197 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:06:09.451348 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:06:10.021065 [debug] [ThreadPool]: SQL status: OK in 0.57 seconds
[0m00:06:10.037765 [debug] [ThreadPool]: On list_schemas: Close
[0m00:06:10.255868 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:06:10.279795 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:10.280188 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:06:10.280464 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:06:10.280722 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:06:10.866323 [debug] [ThreadPool]: SQL status: OK in 0.59 seconds
[0m00:06:10.888937 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:06:10.889373 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:06:11.304545 [debug] [ThreadPool]: SQL status: OK in 0.41 seconds
[0m00:06:11.312605 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:06:11.313496 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:06:11.313977 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:06:11.472634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bda2790>]}
[0m00:06:11.474159 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:11.474915 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:06:11.476230 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:06:11.477232 [info ] [MainThread]: 
[0m00:06:11.485128 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:06:11.486187 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:06:11.487698 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:06:11.488162 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:06:11.493209 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:06:11.494540 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:06:11.488484 => 2023-02-22 00:06:11.494438
[0m00:06:11.495020 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:06:11.543038 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:06:11.543723 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:11.543935 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:06:11.544129 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

-- DROP TABLE IF EXISTS snowflake_silver_users;
-- CREATE TABLE snowflake_silver_users
-- USING snowflake
-- OPTIONS (
--     dbtable 'silver_users',
--     sfUrl 'https://oesmxao-do44125.snowflakecomputing.com',
--     sfUser 'jwszolek',
--     sfPassword '.8kki-cqP@f2W@7zYY46',
--     sfDatabase 'jwszol',
--     sfSchema 'public',
--     sfWarehouse 'COMPUTE_WH'
-- );

select snow.id as snow_id, 
       snow.name as snow_name,
       dbx.id as dbx_id,
       dbx.name as dbx_name,
       dbx.user_id as dbx_user_id
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:06:11.544290 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:06:18.977448 [debug] [Thread-1  ]: SQL status: OK in 7.43 seconds
[0m00:06:19.271848 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:06:11.495345 => 2023-02-22 00:06:19.271749
[0m00:06:19.272338 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:06:19.272634 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:06:19.272880 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:06:19.452249 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '511883b0-2558-4d11-b233-5f3bfc54e695', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf23370>]}
[0m00:06:19.453998 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 7.96s]
[0m00:06:19.457614 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:06:19.461338 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:06:19.462097 [debug] [MainThread]: On master: ROLLBACK
[0m00:06:19.462542 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:06:19.637785 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:06:19.638983 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:06:19.639475 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:06:19.639956 [debug] [MainThread]: On master: ROLLBACK
[0m00:06:19.640412 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:06:19.640855 [debug] [MainThread]: On master: Close
[0m00:06:19.802214 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:06:19.803303 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:06:19.805929 [info ] [MainThread]: 
[0m00:06:19.806955 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.36 seconds (10.36s).
[0m00:06:19.807980 [debug] [MainThread]: Command end result
[0m00:06:19.820696 [info ] [MainThread]: 
[0m00:06:19.821385 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:06:19.821845 [info ] [MainThread]: 
[0m00:06:19.822252 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:06:19.822870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf30f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf04a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcde100>]}
[0m00:06:19.823301 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:10:33.289360 | bcf8684b-313c-4a1f-8335-a9ee96408058 ==============================
[0m00:10:33.289360 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:10:33.291013 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_cities'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:10:33.291914 [debug] [MainThread]: Tracking: tracking
[0m00:10:33.312198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9ea160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9eaf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a9ea880>]}
[0m00:10:33.373440 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m00:10:33.373714 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/gold_users_cities_join.sql
[0m00:10:33.373883 [debug] [MainThread]: Partial parsing: updated file: dbx_cities://models/silver_cities.sql
[0m00:10:33.382541 [debug] [MainThread]: 1699: static parser successfully parsed gold_users_cities_join.sql
[0m00:10:33.389533 [debug] [MainThread]: 1699: static parser successfully parsed silver_cities.sql
[0m00:10:33.396125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab46040>]}
[0m00:10:33.399717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab2efd0>]}
[0m00:10:33.399900 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:10:33.400115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103e2d250>]}
[0m00:10:33.400639 [info ] [MainThread]: 
[0m00:10:33.401535 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:10:33.402066 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:10:33.407493 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:10:33.407720 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:10:33.407847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:10:34.218614 [debug] [ThreadPool]: SQL status: OK in 0.81 seconds
[0m00:10:34.230557 [debug] [ThreadPool]: On list_schemas: Close
[0m00:10:34.386947 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:10:34.403858 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:34.404230 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:10:34.404515 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:10:34.404869 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:10:35.086501 [debug] [ThreadPool]: SQL status: OK in 0.68 seconds
[0m00:10:35.099995 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:10:35.100431 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:10:35.519091 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m00:10:35.524562 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:10:35.525187 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:10:35.525608 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:10:35.685820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abb2a90>]}
[0m00:10:35.695192 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:35.695850 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:10:35.714302 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:10:35.715778 [info ] [MainThread]: 
[0m00:10:35.737770 [debug] [Thread-1  ]: Began running node model.dbx_cities.silver_cities
[0m00:10:35.738342 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.silver_cities ............................. [RUN]
[0m00:10:35.739086 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.silver_cities'
[0m00:10:35.739315 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.silver_cities
[0m00:10:35.741550 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.silver_cities"
[0m00:10:35.742323 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (compile): 2023-02-22 00:10:35.739486 => 2023-02-22 00:10:35.742262
[0m00:10:35.742523 [debug] [Thread-1  ]: Began executing node model.dbx_cities.silver_cities
[0m00:10:35.752571 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:35.752807 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m00:10:35.753005 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

      describe extended `airbyte`.`silver_cities`
  
[0m00:10:35.753173 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:10:36.460791 [debug] [Thread-1  ]: SQL status: OK in 0.71 seconds
[0m00:10:36.506282 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.silver_cities"
[0m00:10:36.506797 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.silver_cities"
[0m00:10:36.506998 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.silver_cities"} */

  
    
        create or replace table `airbyte`.`silver_cities`
      
      
    using delta
      
      
      
      
      
      
      as
      

select id, name, user_id from airbyte.bronze_cities
order by id 
limit 6
  
[0m00:10:40.131825 [debug] [Thread-1  ]: SQL status: OK in 3.62 seconds
[0m00:10:40.158175 [debug] [Thread-1  ]: Timing info for model.dbx_cities.silver_cities (execute): 2023-02-22 00:10:35.742654 => 2023-02-22 00:10:40.158101
[0m00:10:40.158613 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: ROLLBACK
[0m00:10:40.158857 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:10:40.159064 [debug] [Thread-1  ]: On model.dbx_cities.silver_cities: Close
[0m00:10:40.319458 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bcf8684b-313c-4a1f-8335-a9ee96408058', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ada3370>]}
[0m00:10:40.320835 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.silver_cities ........................ [[32mOK[0m in 4.58s]
[0m00:10:40.323537 [debug] [Thread-1  ]: Finished running node model.dbx_cities.silver_cities
[0m00:10:40.325472 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:10:40.325964 [debug] [MainThread]: On master: ROLLBACK
[0m00:10:40.326351 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:10:40.507174 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:10:40.508366 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:10:40.509228 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:10:40.510068 [debug] [MainThread]: On master: ROLLBACK
[0m00:10:40.510583 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:10:40.510939 [debug] [MainThread]: On master: Close
[0m00:10:40.664144 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:10:40.665191 [debug] [MainThread]: Connection 'model.dbx_cities.silver_cities' was properly closed.
[0m00:10:40.668021 [info ] [MainThread]: 
[0m00:10:40.668677 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 7.27 seconds (7.27s).
[0m00:10:40.669580 [debug] [MainThread]: Command end result
[0m00:10:40.681385 [info ] [MainThread]: 
[0m00:10:40.681816 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:10:40.682183 [info ] [MainThread]: 
[0m00:10:40.682602 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:10:40.683113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036a2f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf5220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf5280>]}
[0m00:10:40.683464 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:12:59.089705 | 67fb0201-24e8-451f-b812-389a1382a758 ==============================
[0m00:12:59.089705 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:12:59.091527 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['silver_users'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:12:59.091781 [debug] [MainThread]: Tracking: tracking
[0m00:12:59.103613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152828190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152828fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152828610>]}
[0m00:12:59.157611 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:12:59.157799 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:12:59.161331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67fb0201-24e8-451f-b812-389a1382a758', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152adbf40>]}
[0m00:12:59.165285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67fb0201-24e8-451f-b812-389a1382a758', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a1ac10>]}
[0m00:12:59.165480 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:12:59.165694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67fb0201-24e8-451f-b812-389a1382a758', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107160100>]}
[0m00:12:59.166012 [warn ] [MainThread]: The selection criterion 'silver_users' does not match any nodes
[0m00:12:59.166482 [info ] [MainThread]: 
[0m00:12:59.166742 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m00:12:59.166959 [debug] [MainThread]: Command end result
[0m00:12:59.170163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10715f8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a1a9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152a1a100>]}
[0m00:12:59.170321 [debug] [MainThread]: Flushing usage events


============================== 2023-02-22 00:13:07.469327 | 2a334cc2-e262-467e-8576-df2fde2000cc ==============================
[0m00:13:07.469327 [info ] [MainThread]: Running with dbt=1.4.1
[0m00:13:07.471492 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/kubaw/.dbt', 'send_anonymous_usage_stats': True, 'quiet': False, 'no_print': False, 'cache_selected_only': False, 'select': ['gold_users_cities_join'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m00:13:07.471764 [debug] [MainThread]: Tracking: tracking
[0m00:13:07.487455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9971c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d997fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d997640>]}
[0m00:13:07.544828 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:13:07.545017 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:13:07.548458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11db1cf40>]}
[0m00:13:07.552136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da5bc40>]}
[0m00:13:07.552331 [info ] [MainThread]: Found 4 models, 4 tests, 0 snapshots, 0 analyses, 372 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m00:13:07.552547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a5df0>]}
[0m00:13:07.553075 [info ] [MainThread]: 
[0m00:13:07.553925 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:13:07.554412 [debug] [ThreadPool]: Acquiring new databricks connection 'list_schemas'
[0m00:13:07.560259 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m00:13:07.560516 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m00:13:07.560661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:13:08.426546 [debug] [ThreadPool]: SQL status: OK in 0.87 seconds
[0m00:13:08.444983 [debug] [ThreadPool]: On list_schemas: Close
[0m00:13:08.677035 [debug] [ThreadPool]: Acquiring new databricks connection 'list_None_airbyte'
[0m00:13:08.697973 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:08.698361 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:13:08.698642 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show tables in `airbyte`
  
[0m00:13:08.698897 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m00:13:09.344534 [debug] [ThreadPool]: SQL status: OK in 0.65 seconds
[0m00:13:09.358662 [debug] [ThreadPool]: Using databricks connection "list_None_airbyte"
[0m00:13:09.359062 [debug] [ThreadPool]: On list_None_airbyte: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "connection_name": "list_None_airbyte"} */
show views in `airbyte`
  
[0m00:13:09.775164 [debug] [ThreadPool]: SQL status: OK in 0.42 seconds
[0m00:13:09.781887 [debug] [ThreadPool]: On list_None_airbyte: ROLLBACK
[0m00:13:09.782509 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m00:13:09.782940 [debug] [ThreadPool]: On list_None_airbyte: Close
[0m00:13:09.968209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da5be50>]}
[0m00:13:09.969271 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:09.969744 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:09.970646 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:13:09.971222 [info ] [MainThread]: 
[0m00:13:09.976414 [debug] [Thread-1  ]: Began running node model.dbx_cities.gold_users_cities_join
[0m00:13:09.976909 [info ] [Thread-1  ]: 1 of 1 START sql table model airbyte.gold_users_cities_join .................... [RUN]
[0m00:13:09.977839 [debug] [Thread-1  ]: Acquiring new databricks connection 'model.dbx_cities.gold_users_cities_join'
[0m00:13:09.978149 [debug] [Thread-1  ]: Began compiling node model.dbx_cities.gold_users_cities_join
[0m00:13:09.981212 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbx_cities.gold_users_cities_join"
[0m00:13:09.982568 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (compile): 2023-02-22 00:13:09.978368 => 2023-02-22 00:13:09.982503
[0m00:13:09.982903 [debug] [Thread-1  ]: Began executing node model.dbx_cities.gold_users_cities_join
[0m00:13:09.996771 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:09.996951 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:13:09.997131 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

      describe extended `airbyte`.`gold_users_cities_join`
  
[0m00:13:09.997283 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m00:13:10.678809 [debug] [Thread-1  ]: SQL status: OK in 0.68 seconds
[0m00:13:10.722726 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbx_cities.gold_users_cities_join"
[0m00:13:10.723545 [debug] [Thread-1  ]: Using databricks connection "model.dbx_cities.gold_users_cities_join"
[0m00:13:10.723758 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: /* {"app": "dbt", "dbt_version": "1.4.1", "dbt_databricks_version": "1.4.1", "databricks_sql_connector_version": "2.3.0", "profile_name": "dbx_cities", "target_name": "dev", "node_id": "model.dbx_cities.gold_users_cities_join"} */

  
    
        create or replace table `airbyte`.`gold_users_cities_join`
      
      
    using delta
      
      
      
      
      
      
      as
      

select snow.id as snow_id, 
       snow.name as snow_name,
       dbx.id as dbx_id,
       dbx.name as dbx_name,
       dbx.user_id as dbx_user_id
from hive_metastore.default.snowflake_silver_users snow
join airbyte.silver_cities dbx on snow.id == dbx.user_id;
  
[0m00:13:19.343147 [debug] [Thread-1  ]: SQL status: OK in 8.62 seconds
[0m00:13:19.603409 [debug] [Thread-1  ]: Timing info for model.dbx_cities.gold_users_cities_join (execute): 2023-02-22 00:13:09.983156 => 2023-02-22 00:13:19.603344
[0m00:13:19.603834 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: ROLLBACK
[0m00:13:19.604072 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m00:13:19.604278 [debug] [Thread-1  ]: On model.dbx_cities.gold_users_cities_join: Close
[0m00:13:19.750951 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a334cc2-e262-467e-8576-df2fde2000cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11df8c100>]}
[0m00:13:19.752323 [info ] [Thread-1  ]: 1 of 1 OK created sql table model airbyte.gold_users_cities_join ............... [[32mOK[0m in 9.77s]
[0m00:13:19.755478 [debug] [Thread-1  ]: Finished running node model.dbx_cities.gold_users_cities_join
[0m00:13:19.758200 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:13:19.758867 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:19.759456 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:13:19.951674 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:19.953323 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:19.954067 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:19.954478 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:19.954833 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:19.955181 [debug] [MainThread]: On master: Close
[0m00:13:20.105972 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:13:20.106929 [debug] [MainThread]: Connection 'model.dbx_cities.gold_users_cities_join' was properly closed.
[0m00:13:20.109043 [info ] [MainThread]: 
[0m00:13:20.109913 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 12.56 seconds (12.56s).
[0m00:13:20.110673 [debug] [MainThread]: Command end result
[0m00:13:20.120355 [info ] [MainThread]: 
[0m00:13:20.120904 [info ] [MainThread]: [32mCompleted successfully[0m
[0m00:13:20.121337 [info ] [MainThread]: 
[0m00:13:20.121720 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m00:13:20.122192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054a5df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbcb520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11df13fa0>]}
[0m00:13:20.122560 [debug] [MainThread]: Flushing usage events
